{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd291487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(rank):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)  # CIFAR10 has 10 classes\n",
    "\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if dist.is_initialized():\n",
    "        model = DDP(model, device_ids=[rank] if torch.cuda.is_available() else None)\n",
    "\n",
    "    return model, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloader(rank, world_size, batch_size=64):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)               if world_size > 1 else None\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=(sampler is None)\n",
    "    )\n",
    "\n",
    "    return train_loader, sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(rank, world_size, return_dict):\n",
    "\n",
    "    if world_size > 1:\n",
    "        setup(rank, world_size)\n",
    "\n",
    "    model, device = get_model(rank)\n",
    "    train_loader, sampler = get_dataloader(rank, world_size)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1):  # 1 epoch for demo\n",
    "        if sampler:\n",
    "            sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if rank == 0:\n",
    "        return_dict[\"time\"] = total_time\n",
    "\n",
    "    if world_size > 1:\n",
    "        cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(world_size):\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    if world_size > 1:\n",
    "        mp.spawn(train, args=(world_size, return_dict), nprocs=world_size, join=True)\n",
    "    else:\n",
    "        train(0, 1, return_dict)\n",
    "\n",
    "    return return_dict[\"time\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running single process baseline...\")\n",
    "    t1 = run_experiment(1)\n",
    "    print(\"Time (1 process):\", t1)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        world_size = torch.cuda.device_count()\n",
    "        print(f\"Running distributed with {world_size} processes...\")\n",
    "        tN = run_experiment(world_size)\n",
    "        print(\"Time (N processes):\", tN)\n",
    "\n",
    "        speedup = t1 / tN\n",
    "        print(\"Speedup:\", speedup)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
